# NSU Scrapper

Многоисточниковый скраппер для сбора информационного контента Новосибирского государственного университета (НГУ) из разных источников.

Проект собирает образовательный контент НГУ из **двух источников**:

1. **ВКонтакте** - посты из официальной группы университета
2. **Веб-сайт НГУ** - образовательные страницы, программы, информацию о факультетах

Собранные данные сохраняются в соответствующих файлах в формате `JSONL` для последующей обработки и анализа.

После сбора данных происходит объединение двух jsonl файлов в единный jsonl

---

## Структура проекта

```
Scrapper/
├── crawlers/
│   ├── crawl_nsu_vk_knowledge.py               # Скраппер группы ВКонтакте (берет ссылки из vk_urls.json, генерирует vk_scrapped_<date_1>_to_<date_2>.jsonl)
│   └── crawl_nsu_web_knowledge.py              # Скраппер веб-сайта (берет ссылки из web_urls.json, генерирует web_scrapped_<date>.jsonl)
├── scrapped_data/
│   ├── vk_scrapped_<date_1>_to_<date_2>.jsonl  # Собранные посты из ВК в период с <date_1> по <date_2>
│   ├── web_scrapped_<date>.jsonl               # Собранный контент с веб-сайта c датой сбора <date>
│   ├── merged_latest_knowledge.jsonl           # Объединение vk_scrpped и web_scrapped (Если есть несколько vk_scrapped/web_scrapped, то берём те, у которых date_2/date новее)
│   └── filtered_merged_latest_knowledge.jsonl  # Записи из merged_latest_knowledge.jsonl, прошедшие фильтрацию и трансформацию
├── urls/
│   ├── vk_urls.json                            # Список ВК групп для сбора информации
│   └── web_urls.json                           # Список веб-страниц НГУ для сбора информации
├── .env.sample                                 # Пример .env, значения которых надо будет задать
├── .env                                        # Список используемых переменных окружений, используемых scrapper-ом
├── config.yaml                                 # Конфигурация для скраппера, которую задаёт пользователь
├── default_config.yaml                         # Значение каждого параметра в конфигурации по умолчанию (берётся значение, если пользователь не указал значение в config.yaml)
├── filter_knowledge.py                         # Скрипт для фильтрации и трансформация собранных данных
├── merge_knowledge.py                          # Скрипт для объединения последних собранных данных с разных источников 
└── nsu_urls_spider.py                          # Паук для поиска URLs на сайте НГУ (использовался для получения части ссылок из web_urls.json)
```

## Установка

### Требования

- Python 3.13+
- [uv](https://docs.astral.sh/uv/) - пакетный менеджер и координатор рабочего окружения

### Шаги установки

1. **Установите зависимости**
   ```bash
   uv sync
   ```

   Для установки зависимостей разработки (включая Jupyter):
   ```bash
   uv sync --all-extras
   ```

2. **Активируйте виртуальное окружение**
   ```bash
   source .venv/bin/activate
   ```

4. **Создайте `.env` файл из шаблона**
   ```bash
   cp .env.sample .env
   ```

   Отредактируйте `.env` и добавьте необходимые переменные:
   - `VK_SERVICE_TOKEN` - сервисный токен VK API (получите на https://vk.com/dev)
