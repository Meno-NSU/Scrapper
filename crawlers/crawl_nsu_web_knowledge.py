import json
import logging
from pathlib import Path
import asyncio
from crawl4ai import *
from tqdm import tqdm
import warnings
import time
import datetime

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
logger = logging.getLogger(__name__)

def _extract_urls(urls_fname: Path) -> dict[str, str]:
    with urls_fname.open(mode="r", encoding="utf-8", errors="ignore") as fp:
        url_data = json.load(fp)

    url_dict = {}

    for doc_name, doc_url in url_data.items():
        # –ï—Å–ª–∏ URL –µ—â–µ –Ω–µ—Ç –∏–ª–∏ —Ç–µ–∫—É—â–µ–µ –∏–º—è –¥–ª–∏–Ω–Ω–µ–µ —Ç–æ–≥–æ, —á—Ç–æ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        if doc_url not in url_dict or len(doc_name) > len(url_dict[doc_url]):
            url_dict[doc_url] = doc_name

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
    for doc_url in url_dict:
        url_dict[doc_url] = " ".join(url_dict[doc_url].strip().split()).strip()

    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(url_dict)} url")
    return url_dict

def get_configs():
    browser_config = BrowserConfig(verbose=False)
    run_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.6),
            options={"ignore_links": True},
        ),
        word_count_threshold=10,  # Minimum words per content block
        excluded_tags=["form", "header"],
        exclude_external_links=True,  # Remove external links
        remove_overlay_elements=True,  # Remove popups/modals
        process_iframes=True,
        verbose=False,
    )

    return {"browser": browser_config, "run": run_config}


async def crawl_web_knowledge(url_fname: Path, output: Path, configs: dict):
    success_count = 0
    fail_count = 0

    # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ urls –∏–∑ json —Ñ–∞–π–ª–∞
    url_dict = _extract_urls(url_fname)
    url_list = sorted(list(url_dict.keys()))

    # 2. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    with open(output, mode="w", encoding="utf-8") as fp:
        async with AsyncWebCrawler(config=configs["browser"]) as crawler:
            for doc_url in tqdm(url_list, desc=f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å Web –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"):
                try:
                    result = await crawler.arun(url=doc_url, config=configs["run"])

                    if result.success:
                        jsonified_result = {
                            "url": doc_url,
                            "name": url_dict[doc_url],
                            "content": result.markdown.fit_markdown,
                            "date": None,  # –î–ª—è –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —á–∞—Å—Ç–æ –Ω–µ—Ç —è–≤–Ω–æ–π –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                            "collection_date": int(time.time()),
                        }
                        fp.write(
                            json.dumps(jsonified_result, ensure_ascii=False) + "\n"
                        )
                        fp.flush()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–∞–∑—É
                        success_count += 1
                    else:
                        fail_count += 1
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –Ω–æ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫—Ä–∏–ø—Ç
                        warnings.warn(
                            f"FAIL {doc_url}: Status={result.status_code}, Error={result.error_message}"
                        )
                except Exception as e:
                    fail_count += 1
                    logger.info(f"EXCEPTION {doc_url}: {e}")

    # 3. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    logger.info("-" * 40)
    logger.info(f"üéâ –ì–æ—Ç–æ–≤–æ!")
    logger.info(f"–í—Å–µ–≥–æ URLs: {len(url_list)}")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}")
    if fail_count > 0:
        logger.info(f"‚ö†Ô∏è –û—à–∏–±–æ–∫: {fail_count} (—Å–º. –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –≤—ã—à–µ)")
    else:
        logger.info(f"–û—à–∏–±–æ–∫: 0")
        
    logger.info(f"–§–∞–π–ª: {output}")

async def main():
    BASE = Path(__file__).resolve().parent.parent

    RESOURCES_DIR = BASE.joinpath("urls")
    SCRAPPED_DATA_DIR = BASE.joinpath("scrapped_data")
    logger.info(f"isdir({RESOURCES_DIR}) = {RESOURCES_DIR.is_dir()}")
    logger.info(f"isdir({SCRAPPED_DATA_DIR}) = {SCRAPPED_DATA_DIR.is_dir()}")

    url_fname = RESOURCES_DIR.joinpath("web_urls.json")
    logger.info(f"isfile({url_fname}) = {url_fname.is_file()}")

    # 1. –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    filename = f"web_scrapped_{current_date}.jsonl"
    output = SCRAPPED_DATA_DIR.joinpath(filename)

    await crawl_web_knowledge(url_fname, output, get_configs())

if __name__ == "__main__":
    asyncio.run(main())
